{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b92a4b0f-c1d3-424c-a46b-97b07f57d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, infer_device\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab36a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image_batches(images_folder, batch_size, sample_size=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Yield batches of images directly from a given folder.\n",
    "\n",
    "    Args:\n",
    "        images_folder (str | Path): Path to folder containing images (searched recursively).\n",
    "        batch_size (int): Number of images per batch.\n",
    "        sample_size (int, optional): Limit total number of images to sample.\n",
    "        random_state (int, optional): Seed for reproducible shuffling if needed.\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\")\n",
    "    paths = [p for p in Path(images_folder).glob(\"*\") if p.suffix.lower() in exts]\n",
    "    print(f\"Found {len(paths)} image files\")\n",
    "\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(paths)\n",
    "    if sample_size:\n",
    "        paths = paths[:sample_size]\n",
    "\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch = paths[i:i + batch_size]\n",
    "        yield [Image.open(p).convert(\"RGB\").copy() for p in batch], [p.name for p in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23220f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 image files\n",
      "Batch filenames: ['2023_08_02__13_32_31_894227000___02Z8R__02Z8W_bunch_1.png', '2023_08_02__14_21_25_793843000___02Z8R__02Z8W_bunch_6.png']\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "test_root = \"../Data/tomatoes/images/val\"   # adjust to your dataset path\n",
    "for imgs, names in retrieve_image_batches(test_root, batch_size=2, sample_size=4):\n",
    "    print(\"Batch filenames:\", names)\n",
    "    print(\"Batch size:\", len(imgs))\n",
    "    break  # only first batch for quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6b65d9a-9962-420f-9437-56178225c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name):\n",
    "    if model_name == \"gd_t\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"gd_b\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_t\":\n",
    "        model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_b_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_base_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_l_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_large_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"])\n",
    "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"]).to(device)\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6327152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    images_folder,\n",
    "    categories_list,\n",
    "    model_name,\n",
    "    batch_size=4,\n",
    "    sample_size=20,\n",
    "    random_state=44,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run zero-shot object detection and export results in COCO format.\n",
    "    Saves '<model_name>_<dataset_name>_predictions.json' inside the Results folder under Experiment_1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images_folder : str\n",
    "        Folder containing images.\n",
    "    categories_list : list[str]\n",
    "        Class names (e.g. [\"cat\", \"dog\", \"person\"]).\n",
    "    model_name : str\n",
    "        Model identifier for select_model().\n",
    "    \"\"\"\n",
    "\n",
    "    processor, model = select_model(model_name)\n",
    "\n",
    "    # I/O setup\n",
    "    base = os.path.splitext(os.path.basename(images_folder))[0]\n",
    "    out_dir = os.path.join(\"..\", \"Results\", \"Experiment_1\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_file = os.path.join(out_dir, f\"{model_name}_{base}_predictions.json\")\n",
    "\n",
    "    # build id/name mapping automatically\n",
    "    categories = [{\"id\": i + 1, \"name\": name} for i, name in enumerate(categories_list)]\n",
    "    categories_dict = {name: i + 1 for i, name in enumerate(categories_list)}\n",
    "\n",
    "    info = {\n",
    "        \"description\": f\"Predictions on {images_folder} with {model_name}\",\n",
    "        \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    images, annotations = [], []\n",
    "\n",
    "    def image_id_from_name(name: str) -> int:\n",
    "        return int(hashlib.md5(name.encode()).hexdigest()[:8], 16)\n",
    "\n",
    "    model.eval()\n",
    "    use_cuda = (model.device.type == \"cuda\")\n",
    "\n",
    "    label_list = list(categories_dict.keys())\n",
    "    cached_text = processor(text=[label_list], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    total_time = 0.0\n",
    "    num_images = 0\n",
    "    num_boxes = 0\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(2):\n",
    "        dummy = processor(\n",
    "            images=[Image.new(\"RGB\", (224, 224))] * batch_size,\n",
    "            text=[label_list] * batch_size,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            _ = model(**dummy)\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    cached_text = None\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, names in retrieve_image_batches(\n",
    "            images_folder=images_folder,\n",
    "            batch_size=batch_size,\n",
    "            sample_size=sample_size,\n",
    "            random_state=random_state,\n",
    "        ):\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            if cached_text is None:\n",
    "                cached_text = processor(text=[label_list], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\", padding=True)\n",
    "            inputs[\"input_ids\"] = cached_text.input_ids.repeat(len(imgs), 1)\n",
    "            inputs = {k: v.to(model.device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            target_sizes = [(im.height, im.width) for im in imgs]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs[\"input_ids\"],\n",
    "                threshold=threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            total_time += (end - start)\n",
    "            num_images += len(imgs)\n",
    "\n",
    "            for name, res, im in zip(names, results, imgs):\n",
    "                img_id = image_id_from_name(name)\n",
    "                H, W = im.height, im.width\n",
    "                images.append({\"id\": img_id, \"file_name\": f\"images/val/{name}\"})\n",
    "\n",
    "                boxes = res[\"boxes\"].tolist()\n",
    "                scores = res[\"scores\"].tolist()\n",
    "                labels = res.get(\"text_labels\", [])\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = map(float, box)\n",
    "\n",
    "                    # clamp to image bounds\n",
    "                    x1 = max(0.0, min(x1, W))\n",
    "                    y1 = max(0.0, min(y1, H))\n",
    "                    x2 = max(0.0, min(x2, W))\n",
    "                    y2 = max(0.0, min(y2, H))\n",
    "\n",
    "                    # enforce proper ordering\n",
    "                    if x2 < x1:\n",
    "                        x1, x2 = x2, x1\n",
    "                    if y2 < y1:\n",
    "                        y1, y2 = y2, y1\n",
    "\n",
    "                    w = max(0.0, x2 - x1)\n",
    "                    h = max(0.0, y2 - y1)\n",
    "                    if w == 0.0 or h == 0.0:\n",
    "                        continue\n",
    "\n",
    "                    cid = categories_dict.get(label)\n",
    "                    if cid is None:\n",
    "                        continue\n",
    "\n",
    "                    ann_id = len(annotations) + 1\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": cid,\n",
    "                        \"bbox\": [x1, y1, w, h],\n",
    "                        \"score\": float(score),\n",
    "                    })\n",
    "                    num_boxes += 1\n",
    "\n",
    "    avg_time_image = total_time / num_images if num_images else 0.0\n",
    "    avg_time_bbox = total_time / num_boxes if num_boxes else 0.0\n",
    "\n",
    "    info.update({\n",
    "        \"num_images\": num_images,\n",
    "        \"num_predicted_bbox\": num_boxes,\n",
    "        \"avg_inference_time_s_image\": avg_time_image,\n",
    "        \"avg_inference_time_s_bbox\": avg_time_bbox,\n",
    "        \"total_inference_time_s\": total_time,\n",
    "    })\n",
    "\n",
    "    coco_output = {\n",
    "        \"info\": info,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Wrote COCO-format JSON to {out_file}\")\n",
    "    print(f\"Total inference time: {total_time:.4f}s\")\n",
    "    print(f\"Avg inference time per image: {avg_time_image:.4f}s\")\n",
    "    print(f\"Avg inference time per bbox: {avg_time_bbox:.6f}s\")\n",
    "    print(f\"Processed {num_images} images and {num_boxes} boxes\")\n",
    "\n",
    "    return coco_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38602792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 image files\n",
      "Wrote COCO-format JSON to ..\\Results\\Experiment_1\\gd_t_val_predictions.json\n",
      "Total inference time: 519.9467s\n",
      "Avg inference time per image: 25.9973s\n",
      "Avg inference time per bbox: 17.929196s\n",
      "Processed 20 images and 29 boxes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'info': {'description': 'Predictions on ../Data/tomatoes/images/val with gd_t',\n",
       "  'date_created': '2025-11-11 18:31:03',\n",
       "  'num_images': 20,\n",
       "  'num_predicted_bbox': 29,\n",
       "  'avg_inference_time_s_image': 25.997333549996256,\n",
       "  'avg_inference_time_s_bbox': 17.929195551721556,\n",
       "  'total_inference_time_s': 519.9466709999251},\n",
       " 'images': [{'id': 1467614578,\n",
       "   'file_name': 'images/val/2023_08_11__16_20_43_969564000___02Z8R__02Z8W_bunch_6.png'},\n",
       "  {'id': 3405516372,\n",
       "   'file_name': 'images/val/2023_08_18__08_39_27_579884000___02Z8R__02Z8W_bunch_4.png'},\n",
       "  {'id': 796568874,\n",
       "   'file_name': 'images/val/2023_08_18__13_17_47_649299000___02Z8R__02Z8W_bunch_5.png'},\n",
       "  {'id': 4067744067,\n",
       "   'file_name': 'images/val/2023_08_04__08_31_03_412536000___02Z8R__02Z8W_bunch_3.png'},\n",
       "  {'id': 309385764,\n",
       "   'file_name': 'images/val/2023_11_22__15_06_41_214000000___04ZXJ__04ZXK_bunch_4.png'},\n",
       "  {'id': 755751560,\n",
       "   'file_name': 'images/val/2024_09_17__12_45_33_925381000___cam3__cam4_bunch_0.png'},\n",
       "  {'id': 4020469717,\n",
       "   'file_name': 'images/val/2023_11_22__15_15_07_526000000___04Z49__04T2Y_bunch_10.png'},\n",
       "  {'id': 2406094948,\n",
       "   'file_name': 'images/val/2023_11_22__14_57_50_566000000___04Z49__04T2Y_bunch_1.png'},\n",
       "  {'id': 3334813146,\n",
       "   'file_name': 'images/val/2023_08_04__12_59_19_658482000___02Z8R__02Z8W_bunch_0.png'},\n",
       "  {'id': 2923585349,\n",
       "   'file_name': 'images/val/2024_01_17__11_57_56_662000000___04Z49__04T2Y_bunch_2.png'},\n",
       "  {'id': 1541244819,\n",
       "   'file_name': 'images/val/2023_11_22__15_20_52_942000000___04Z49__04T2Y_bunch_6.png'},\n",
       "  {'id': 2760845407,\n",
       "   'file_name': 'images/val/2024_01_17__12_17_08_173000000___04ZXJ__04ZXK_bunch_12.png'},\n",
       "  {'id': 1381631878,\n",
       "   'file_name': 'images/val/2023_08_04__08_24_54_154446000___02Z8R__02Z8W_bunch_1.png'},\n",
       "  {'id': 3155059531,\n",
       "   'file_name': 'images/val/2025_02_06__13_25_15_163000000___camera_4__camera_5_bunch_4.png'},\n",
       "  {'id': 2865569875,\n",
       "   'file_name': 'images/val/2025_02_06__13_25_42_214000000___camera_0__camera_1_bunch_10.png'},\n",
       "  {'id': 4169731163,\n",
       "   'file_name': 'images/val/2024_12_18__13_01_55_156261000___cam1__cam2_bunch_2.png'},\n",
       "  {'id': 2910229138,\n",
       "   'file_name': 'images/val/2024_01_17__11_57_36_045000000___04ZXJ__04ZXK_bunch_1.png'},\n",
       "  {'id': 1128414035,\n",
       "   'file_name': 'images/val/2023_11_22__14_54_57_259000000___04ZXJ__04ZXK_bunch_1.png'},\n",
       "  {'id': 2012545466,\n",
       "   'file_name': 'images/val/2024_09_17__13_01_29_033718000___cam3__cam4_bunch_0.png'},\n",
       "  {'id': 90600241,\n",
       "   'file_name': 'images/val/2025_02_06__13_18_22_081000000___camera_4__camera_5_bunch_6.png'}],\n",
       " 'annotations': [{'image_id': 3405516372,\n",
       "   'category_id': 1,\n",
       "   'bbox': [65.82331848144531,\n",
       "    114.86583709716797,\n",
       "    35.9830322265625,\n",
       "    38.894081115722656],\n",
       "   'score': 0.44331151247024536},\n",
       "  {'image_id': 3405516372,\n",
       "   'category_id': 1,\n",
       "   'bbox': [61.24079895019531,\n",
       "    173.00845336914062,\n",
       "    53.53527069091797,\n",
       "    56.49519348144531],\n",
       "   'score': 0.4783157706260681},\n",
       "  {'image_id': 796568874,\n",
       "   'category_id': 1,\n",
       "   'bbox': [-0.003550022840499878,\n",
       "    343.0347900390625,\n",
       "    66.9826531112194,\n",
       "    153.26611328125],\n",
       "   'score': 0.42548513412475586},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [45.68164825439453,\n",
       "    43.661930084228516,\n",
       "    88.50940704345703,\n",
       "    79.5578498840332],\n",
       "   'score': 0.6223066449165344},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [107.38230895996094,\n",
       "    94.62471008300781,\n",
       "    89.38632202148438,\n",
       "    74.23629760742188],\n",
       "   'score': 0.6219473481178284},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [22.216049194335938,\n",
       "    95.72753143310547,\n",
       "    79.1990737915039,\n",
       "    75.6384048461914],\n",
       "   'score': 0.6124635934829712},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [83.05146026611328,\n",
       "    154.87547302246094,\n",
       "    70.70012664794922,\n",
       "    67.58346557617188],\n",
       "   'score': 0.5888369679450989},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [15.489092826843262,\n",
       "    153.0561065673828,\n",
       "    53.36508655548096,\n",
       "    67.79127502441406],\n",
       "   'score': 0.5773876905441284},\n",
       "  {'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [115.93804931640625,\n",
       "    29.217195510864258,\n",
       "    76.07296752929688,\n",
       "    47.65846061706543],\n",
       "   'score': 0.41379451751708984},\n",
       "  {'image_id': 2406094948,\n",
       "   'category_id': 1,\n",
       "   'bbox': [81.38027954101562,\n",
       "    166.84120178222656,\n",
       "    47.67417907714844,\n",
       "    48.38499450683594],\n",
       "   'score': 0.40575090050697327},\n",
       "  {'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [80.1658706665039,\n",
       "    151.375732421875,\n",
       "    78.66036224365234,\n",
       "    80.85700988769531],\n",
       "   'score': 0.6004424691200256},\n",
       "  {'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [64.6167221069336,\n",
       "    80.39937591552734,\n",
       "    80.85655975341797,\n",
       "    75.0431900024414],\n",
       "   'score': 0.5961016416549683},\n",
       "  {'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [57.07329177856445,\n",
       "    220.5530242919922,\n",
       "    81.11249923706055,\n",
       "    85.22773742675781],\n",
       "   'score': 0.6361998319625854},\n",
       "  {'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [12.345598220825195,\n",
       "    23.156801223754883,\n",
       "    79.9499568939209,\n",
       "    71.35291862487793],\n",
       "   'score': 0.5463741421699524},\n",
       "  {'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [76.30730438232422,\n",
       "    18.283645629882812,\n",
       "    64.04467010498047,\n",
       "    67.47227478027344],\n",
       "   'score': 0.5132706165313721},\n",
       "  {'image_id': 2923585349,\n",
       "   'category_id': 1,\n",
       "   'bbox': [17.181774139404297,\n",
       "    39.06181716918945,\n",
       "    54.28596115112305,\n",
       "    53.92509841918945],\n",
       "   'score': 0.4241901934146881},\n",
       "  {'image_id': 2923585349,\n",
       "   'category_id': 1,\n",
       "   'bbox': [87.21682739257812,\n",
       "    23.250539779663086,\n",
       "    51.42027282714844,\n",
       "    52.84027290344238],\n",
       "   'score': 0.4051103889942169},\n",
       "  {'image_id': 1541244819,\n",
       "   'category_id': 1,\n",
       "   'bbox': [34.154720306396484,\n",
       "    63.217979431152344,\n",
       "    41.9752082824707,\n",
       "    45.834144592285156],\n",
       "   'score': 0.5680318474769592},\n",
       "  {'image_id': 2865569875,\n",
       "   'category_id': 1,\n",
       "   'bbox': [80.48432159423828,\n",
       "    187.98779296875,\n",
       "    55.021583557128906,\n",
       "    75.63137817382812],\n",
       "   'score': 0.4122791290283203},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [20.920085906982422,\n",
       "    93.73616790771484,\n",
       "    58.18854904174805,\n",
       "    59.793846130371094],\n",
       "   'score': 0.5627259612083435},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [79.7417984008789,\n",
       "    178.2864990234375,\n",
       "    61.57122039794922,\n",
       "    60.68669128417969],\n",
       "   'score': 0.5118744969367981},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [87.00094604492188,\n",
       "    68.05864715576172,\n",
       "    50.835906982421875,\n",
       "    55.46477508544922],\n",
       "   'score': 0.6014094948768616},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [8.888497352600098,\n",
       "    147.81375122070312,\n",
       "    63.47200679779053,\n",
       "    63.879150390625],\n",
       "   'score': 0.5129067897796631},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [79.75837707519531,\n",
       "    122.26329803466797,\n",
       "    51.97406005859375,\n",
       "    59.49394989013672],\n",
       "   'score': 0.5349406003952026},\n",
       "  {'image_id': 4169731163,\n",
       "   'category_id': 1,\n",
       "   'bbox': [24.426267623901367,\n",
       "    209.8926544189453,\n",
       "    61.96658515930176,\n",
       "    55.37590026855469],\n",
       "   'score': 0.4487314820289612},\n",
       "  {'image_id': 2910229138,\n",
       "   'category_id': 1,\n",
       "   'bbox': [63.73944091796875,\n",
       "    30.372465133666992,\n",
       "    67.01585388183594,\n",
       "    77.37254524230957],\n",
       "   'score': 0.4516930878162384},\n",
       "  {'image_id': 2910229138,\n",
       "   'category_id': 1,\n",
       "   'bbox': [71.11483001708984,\n",
       "    100.74964904785156,\n",
       "    72.2237777709961,\n",
       "    78.99931335449219],\n",
       "   'score': 0.40985849499702454},\n",
       "  {'image_id': 1128414035,\n",
       "   'category_id': 1,\n",
       "   'bbox': [23.438047409057617,\n",
       "    96.78060913085938,\n",
       "    53.09898567199707,\n",
       "    55.746978759765625],\n",
       "   'score': 0.4085109829902649},\n",
       "  {'image_id': 90600241,\n",
       "   'category_id': 1,\n",
       "   'bbox': [73.02964782714844,\n",
       "    -0.06390893459320068,\n",
       "    83.32064819335938,\n",
       "    99.1772130727768],\n",
       "   'score': 0.5177858471870422}],\n",
       " 'categories': [{'id': 1, 'name': 'tomato'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(\n",
    "    images_folder=\"../Data/tomatoes/images/val\",\n",
    "    categories_list=[\"tomato\"],\n",
    "    model_name=\"gd_t\",\n",
    "    batch_size=2,\n",
    "    sample_size=20,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5739e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_pr(cocoEval, iou_thr=0.50):\n",
    "    # indices\n",
    "    iou_idx = np.where(np.isclose(cocoEval.params.iouThrs, iou_thr))[0][0]\n",
    "    area_all = cocoEval.params.areaRng[0]  # 'all' area range [0^2, inf^2]\n",
    "    maxDets = cocoEval.params.maxDets[-1]\n",
    "\n",
    "    tp = fp = fn = 0\n",
    "\n",
    "    for ei in cocoEval.evalImgs:\n",
    "        if ei is None:\n",
    "            continue\n",
    "        # keep ONLY area='all'\n",
    "        if not np.allclose(ei[\"aRng\"], area_all):\n",
    "            continue\n",
    "\n",
    "        dtMatches = np.asarray(ei[\"dtMatches\"])  # [T, D]\n",
    "        gtMatches = np.asarray(ei[\"gtMatches\"])  # [T, G]\n",
    "        dtIgnore  = np.asarray(ei[\"dtIgnore\"],  dtype=bool)  # [T, D] or [D]\n",
    "        gtIgnore  = np.asarray(ei[\"gtIgnore\"],  dtype=bool)  # [G]\n",
    "\n",
    "        # Some pycocotools versions store dtIgnore as [D]; normalize\n",
    "        if dtIgnore.ndim == 1:\n",
    "            dtIgnore = np.tile(dtIgnore, (len(cocoEval.params.iouThrs), 1))\n",
    "\n",
    "        dtm = dtMatches[iou_idx, :maxDets]\n",
    "        dti = dtIgnore[iou_idx, :maxDets]\n",
    "        gtm = gtMatches[iou_idx]\n",
    "        gti = gtIgnore\n",
    "\n",
    "        tp += int(np.sum((dtm > 0) & (~dti)))\n",
    "        fp += int(np.sum((dtm == 0) & (~dti)))\n",
    "        fn += int(np.sum((gtm == 0) & (~gti)))\n",
    "\n",
    "    print(f\"true positives:{tp}\")\n",
    "    print(f\"false positives:{fp}\")\n",
    "    print(f\"false negatives:{fn}\")\n",
    "    precision_exact = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall_exact = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    return precision_exact, recall_exact\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
