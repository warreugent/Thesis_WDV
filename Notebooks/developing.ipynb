{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92a4b0f-c1d3-424c-a46b-97b07f57d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoofdgebruiker\\miniconda3\\envs\\thesis-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, infer_device\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab36a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image_batches(images_folder, batch_size, sample_size=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Yield batches of images directly from a given folder.\n",
    "\n",
    "    Args:\n",
    "        images_folder (str | Path): Path to folder containing images (searched recursively).\n",
    "        batch_size (int): Number of images per batch.\n",
    "        sample_size (int, optional): Limit total number of images to sample.\n",
    "        random_state (int, optional): Seed for reproducible shuffling if needed.\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\")\n",
    "    paths = [p for p in Path(images_folder).glob(\"*\") if p.suffix.lower() in exts]\n",
    "    print(f\"Found {len(paths)} image files\")\n",
    "\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(paths)\n",
    "    if sample_size:\n",
    "        paths = paths[:sample_size]\n",
    "\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch = paths[i:i + batch_size]\n",
    "        yield [Image.open(p).convert(\"RGB\").copy() for p in batch], [p.name for p in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23220f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 image files\n",
      "Batch filenames: ['2023_08_02__13_32_31_894227000___02Z8R__02Z8W_bunch_1.png', '2023_08_02__14_21_25_793843000___02Z8R__02Z8W_bunch_6.png']\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "test_root = \"../Data/tomatoes/images/val\"   # adjust to your dataset path\n",
    "for imgs, names in retrieve_image_batches(test_root, batch_size=2, sample_size=4):\n",
    "    print(\"Batch filenames:\", names)\n",
    "    print(\"Batch size:\", len(imgs))\n",
    "    break  # only first batch for quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b65d9a-9962-420f-9437-56178225c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name):\n",
    "    if model_name == \"gd_t\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"gd_b\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_t\":\n",
    "        model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_b_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_base_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_l_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_large_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"])\n",
    "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"]).to(device)\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6327152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    images_folder,\n",
    "    categories_list,\n",
    "    model_name,\n",
    "    batch_size=4,\n",
    "    sample_size=20,\n",
    "    random_state=44,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    # ---- cost / energy assumptions ----\n",
    "    gpu_hourly_price=2.5,          # €/GPU-hour (infrastructure)\n",
    "    gpu_tdp_watts=250.0,           # GPU TDP or avg draw in watts\n",
    "    gpu_utilization_factor=0.9,    # fraction of wall time GPU is actively used\n",
    "    power_utilization_factor=0.7,  # fraction of TDP actually drawn\n",
    "    electricity_price_per_kwh=0.30 # €/kWh\n",
    "):\n",
    "    \"\"\"\n",
    "    Run zero-shot object detection and export results in COCO format.\n",
    "    Saves '<model_name>_<dataset_name>_predictions.json' inside the Results folder under Experiment_1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images_folder : str\n",
    "        Folder containing images.\n",
    "    categories_list : list[str]\n",
    "        Class names (e.g. [\"cat\", \"dog\", \"person\"]).\n",
    "    model_name : str\n",
    "        Model identifier for select_model().\n",
    "    \"\"\"\n",
    "\n",
    "    processor, model = select_model(model_name)\n",
    "\n",
    "    # I/O setup\n",
    "    parent = os.path.basename(os.path.dirname(os.path.dirname(images_folder)))\n",
    "    base = os.path.splitext(os.path.basename(images_folder))[0]\n",
    "    out_dir = os.path.join(\"..\", \"Results\", \"Experiment_1\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_file = os.path.join(out_dir, f\"{parent}_{base}_{model_name}_predictions.json\")\n",
    "\n",
    "    # build id/name mapping automatically\n",
    "    categories = [{\"id\": i + 1, \"name\": name} for i, name in enumerate(categories_list)]\n",
    "    categories_dict = {name: i + 1 for i, name in enumerate(categories_list)}\n",
    "\n",
    "    info = {\n",
    "        \"description\": f\"Predictions on {images_folder} with {model_name}\",\n",
    "        \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    images, annotations = [], []\n",
    "\n",
    "    def image_id_from_name(name: str) -> int:\n",
    "        return int(hashlib.md5(name.encode()).hexdigest()[:8], 16)\n",
    "\n",
    "    model.eval()\n",
    "    use_cuda = (model.device.type == \"cuda\")\n",
    "\n",
    "    label_list = list(categories_dict.keys())\n",
    "    cached_text = None  # will be created lazily to match the actual run\n",
    "\n",
    "    total_time = 0.0\n",
    "    num_images = 0\n",
    "    num_boxes = 0\n",
    "\n",
    "    warmup_steps = 2\n",
    "    first_batch = True\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, names in retrieve_image_batches(\n",
    "            images_folder=images_folder,\n",
    "            batch_size=batch_size,\n",
    "            sample_size=sample_size,\n",
    "            random_state=random_state,\n",
    "        ):\n",
    "            # ensure text encoding is built once and reused\n",
    "            if cached_text is None:\n",
    "                cached_text = processor(text=[label_list], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "            # build inputs for this batch\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\", padding=True)\n",
    "            inputs[\"input_ids\"] = cached_text.input_ids.repeat(len(imgs), 1)\n",
    "            inputs = {k: v.to(model.device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "            # warmup on the first real batch (not timed, not counted)\n",
    "            if first_batch:\n",
    "                for _ in range(warmup_steps):\n",
    "                    _ = model(**inputs)\n",
    "                    if use_cuda:\n",
    "                        torch.cuda.synchronize()\n",
    "                first_batch = False  # from now on: only timed runs\n",
    "\n",
    "            # timed forward pass\n",
    "            start = time.perf_counter()\n",
    "            outputs = model(**inputs)\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            target_sizes = [(im.height, im.width) for im in imgs]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs[\"input_ids\"],\n",
    "                threshold=threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            total_time += (end - start)\n",
    "            num_images += len(imgs)\n",
    "\n",
    "            for name, res, im in zip(names, results, imgs):\n",
    "                img_id = image_id_from_name(name)\n",
    "                H, W = im.height, im.width\n",
    "                images.append({\"id\": img_id, \"file_name\": f\"images/val/{name}\"})\n",
    "\n",
    "                boxes = res[\"boxes\"].tolist()\n",
    "                scores = res[\"scores\"].tolist()\n",
    "                labels = res.get(\"text_labels\", [])\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = map(float, box)\n",
    "\n",
    "                    # clamp to image bounds\n",
    "                    x1 = max(0.0, min(x1, W))\n",
    "                    y1 = max(0.0, min(y1, H))\n",
    "                    x2 = max(0.0, min(x2, W))\n",
    "                    y2 = max(0.0, min(y2, H))\n",
    "\n",
    "                    # enforce proper ordering\n",
    "                    if x2 < x1:\n",
    "                        x1, x2 = x2, x1\n",
    "                    if y2 < y1:\n",
    "                        y1, y2 = y2, y1\n",
    "\n",
    "                    w = max(0.0, x2 - x1)\n",
    "                    h = max(0.0, y2 - y1)\n",
    "                    if w == 0.0 or h == 0.0:\n",
    "                        continue\n",
    "\n",
    "                    cid = categories_dict.get(label)\n",
    "                    if cid is None:\n",
    "                        continue\n",
    "\n",
    "                    ann_id = len(annotations) + 1\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": cid,\n",
    "                        \"bbox\": [x1, y1, w, h],\n",
    "                        \"score\": float(score),\n",
    "                    })\n",
    "                    num_boxes += 1\n",
    "\n",
    "    avg_time_image = total_time / num_images if num_images else 0.0\n",
    "    avg_time_bbox = total_time / num_boxes if num_boxes else 0.0\n",
    "\n",
    "    # ---- cost / energy estimates ----\n",
    "    # Convert wall-clock time to GPU-hours (approximate active time)\n",
    "    gpu_hours = (total_time / 3600.0) * gpu_utilization_factor\n",
    "\n",
    "    # Infrastructure cost (e.g. cloud rental)\n",
    "    infra_cost_eur = gpu_hours * gpu_hourly_price\n",
    "\n",
    "    # Energy consumption in kWh (TDP * time * utilization)\n",
    "    energy_kwh = (gpu_tdp_watts / 1000.0) * (total_time / 3600.0) * power_utilization_factor\n",
    "\n",
    "    # Electricity cost\n",
    "    energy_cost_eur = energy_kwh * electricity_price_per_kwh\n",
    "\n",
    "    # Total estimated cost\n",
    "    total_cost_eur = infra_cost_eur + energy_cost_eur\n",
    "\n",
    "    cost_per_image_eur = total_cost_eur / num_images if num_images else 0.0\n",
    "    cost_per_bbox_eur = total_cost_eur / num_boxes if num_boxes else 0.0\n",
    "\n",
    "    info.update({\n",
    "        \"num_images\": num_images,\n",
    "        \"num_predicted_bbox\": num_boxes,\n",
    "        \"avg_inference_time_s_image\": avg_time_image,\n",
    "        \"avg_inference_time_s_bbox\": avg_time_bbox,\n",
    "        \"total_inference_time_s\": total_time,\n",
    "        # cost / energy\n",
    "        \"gpu_hours_estimate\": gpu_hours,\n",
    "        \"total_cost_eur\": total_cost_eur,\n",
    "        \"cost_per_bbox_eur\": cost_per_bbox_eur,\n",
    "    })\n",
    "\n",
    "    coco_output = {\n",
    "        \"info\": info,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Wrote COCO-format JSON to {out_file}\")\n",
    "    print(f\"Total inference time: {total_time:.4f}s\")\n",
    "    print(f\"Avg inference time per image: {avg_time_image:.4f}s\")\n",
    "    print(f\"Avg inference time per bbox: {avg_time_bbox:.6f}s\")\n",
    "    print(f\"Cost per bbox: {cost_per_bbox_eur:.8f} €\")\n",
    "    print(f\"Total cost: {total_cost_eur:.2f} €\")\n",
    "    print(f\"Processed {num_images} images and {num_boxes} boxes\")\n",
    "\n",
    "    return coco_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38602792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 image files\n",
      "Wrote COCO-format JSON to ..\\Results\\Experiment_1\\tomatoes_val_gd_t_predictions.json\n",
      "Total inference time: 176.4048s\n",
      "Avg inference time per image: 17.6405s\n",
      "Avg inference time per bbox: 10.376754s\n",
      "Processed 10 images and 17 boxes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'info': {'description': 'Predictions on ../Data/tomatoes/images/val with gd_t',\n",
       "  'date_created': '2025-11-11 23:09:30',\n",
       "  'num_images': 10,\n",
       "  'num_predicted_bbox': 17,\n",
       "  'avg_inference_time_s_image': 17.640481490001548,\n",
       "  'avg_inference_time_s_bbox': 10.37675381764797,\n",
       "  'total_inference_time_s': 176.40481490001548},\n",
       " 'images': [{'id': 1467614578,\n",
       "   'file_name': 'images/val/2023_08_11__16_20_43_969564000___02Z8R__02Z8W_bunch_6.png'},\n",
       "  {'id': 3405516372,\n",
       "   'file_name': 'images/val/2023_08_18__08_39_27_579884000___02Z8R__02Z8W_bunch_4.png'},\n",
       "  {'id': 796568874,\n",
       "   'file_name': 'images/val/2023_08_18__13_17_47_649299000___02Z8R__02Z8W_bunch_5.png'},\n",
       "  {'id': 4067744067,\n",
       "   'file_name': 'images/val/2023_08_04__08_31_03_412536000___02Z8R__02Z8W_bunch_3.png'},\n",
       "  {'id': 309385764,\n",
       "   'file_name': 'images/val/2023_11_22__15_06_41_214000000___04ZXJ__04ZXK_bunch_4.png'},\n",
       "  {'id': 755751560,\n",
       "   'file_name': 'images/val/2024_09_17__12_45_33_925381000___cam3__cam4_bunch_0.png'},\n",
       "  {'id': 4020469717,\n",
       "   'file_name': 'images/val/2023_11_22__15_15_07_526000000___04Z49__04T2Y_bunch_10.png'},\n",
       "  {'id': 2406094948,\n",
       "   'file_name': 'images/val/2023_11_22__14_57_50_566000000___04Z49__04T2Y_bunch_1.png'},\n",
       "  {'id': 3334813146,\n",
       "   'file_name': 'images/val/2023_08_04__12_59_19_658482000___02Z8R__02Z8W_bunch_0.png'},\n",
       "  {'id': 2923585349,\n",
       "   'file_name': 'images/val/2024_01_17__11_57_56_662000000___04Z49__04T2Y_bunch_2.png'}],\n",
       " 'annotations': [{'id': 1,\n",
       "   'image_id': 3405516372,\n",
       "   'category_id': 1,\n",
       "   'bbox': [65.82331848144531,\n",
       "    114.86583709716797,\n",
       "    35.9830322265625,\n",
       "    38.894081115722656],\n",
       "   'score': 0.44331151247024536},\n",
       "  {'id': 2,\n",
       "   'image_id': 3405516372,\n",
       "   'category_id': 1,\n",
       "   'bbox': [61.24079895019531,\n",
       "    173.00845336914062,\n",
       "    53.53527069091797,\n",
       "    56.49519348144531],\n",
       "   'score': 0.4783157706260681},\n",
       "  {'id': 3,\n",
       "   'image_id': 796568874,\n",
       "   'category_id': 1,\n",
       "   'bbox': [0.0, 343.0347900390625, 66.9791030883789, 153.26611328125],\n",
       "   'score': 0.42548513412475586},\n",
       "  {'id': 4,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [45.68164825439453,\n",
       "    43.661930084228516,\n",
       "    88.50940704345703,\n",
       "    79.5578498840332],\n",
       "   'score': 0.6223066449165344},\n",
       "  {'id': 5,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [107.38230895996094,\n",
       "    94.62471008300781,\n",
       "    89.38632202148438,\n",
       "    74.23629760742188],\n",
       "   'score': 0.6219473481178284},\n",
       "  {'id': 6,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [22.216049194335938,\n",
       "    95.72753143310547,\n",
       "    79.1990737915039,\n",
       "    75.6384048461914],\n",
       "   'score': 0.6124635934829712},\n",
       "  {'id': 7,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [83.05146026611328,\n",
       "    154.87547302246094,\n",
       "    70.70012664794922,\n",
       "    67.58346557617188],\n",
       "   'score': 0.5888369679450989},\n",
       "  {'id': 8,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [15.489092826843262,\n",
       "    153.0561065673828,\n",
       "    53.36508655548096,\n",
       "    67.79127502441406],\n",
       "   'score': 0.5773876905441284},\n",
       "  {'id': 9,\n",
       "   'image_id': 755751560,\n",
       "   'category_id': 1,\n",
       "   'bbox': [115.93804931640625,\n",
       "    29.217195510864258,\n",
       "    76.07296752929688,\n",
       "    47.65846061706543],\n",
       "   'score': 0.41379451751708984},\n",
       "  {'id': 10,\n",
       "   'image_id': 2406094948,\n",
       "   'category_id': 1,\n",
       "   'bbox': [81.38027954101562,\n",
       "    166.84120178222656,\n",
       "    47.67417907714844,\n",
       "    48.38499450683594],\n",
       "   'score': 0.40575090050697327},\n",
       "  {'id': 11,\n",
       "   'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [80.1658706665039,\n",
       "    151.375732421875,\n",
       "    78.66036224365234,\n",
       "    80.85700988769531],\n",
       "   'score': 0.6004424691200256},\n",
       "  {'id': 12,\n",
       "   'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [64.6167221069336,\n",
       "    80.39937591552734,\n",
       "    80.85655975341797,\n",
       "    75.0431900024414],\n",
       "   'score': 0.5961016416549683},\n",
       "  {'id': 13,\n",
       "   'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [57.07329177856445,\n",
       "    220.5530242919922,\n",
       "    81.11249923706055,\n",
       "    85.22773742675781],\n",
       "   'score': 0.6361998319625854},\n",
       "  {'id': 14,\n",
       "   'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [12.345598220825195,\n",
       "    23.156801223754883,\n",
       "    79.9499568939209,\n",
       "    71.35291862487793],\n",
       "   'score': 0.5463741421699524},\n",
       "  {'id': 15,\n",
       "   'image_id': 3334813146,\n",
       "   'category_id': 1,\n",
       "   'bbox': [76.30730438232422,\n",
       "    18.283645629882812,\n",
       "    64.04467010498047,\n",
       "    67.47227478027344],\n",
       "   'score': 0.5132706165313721},\n",
       "  {'id': 16,\n",
       "   'image_id': 2923585349,\n",
       "   'category_id': 1,\n",
       "   'bbox': [17.181774139404297,\n",
       "    39.06181716918945,\n",
       "    54.28596115112305,\n",
       "    53.92509841918945],\n",
       "   'score': 0.4241901934146881},\n",
       "  {'id': 17,\n",
       "   'image_id': 2923585349,\n",
       "   'category_id': 1,\n",
       "   'bbox': [87.21682739257812,\n",
       "    23.250539779663086,\n",
       "    51.42027282714844,\n",
       "    52.84027290344238],\n",
       "   'score': 0.4051103889942169}],\n",
       " 'categories': [{'id': 1, 'name': 'tomato'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_predictions(\n",
    "    images_folder=\"../Data/tomatoes/images/val\",\n",
    "    categories_list=[\"tomato\"],\n",
    "    model_name=\"gd_t\",\n",
    "    batch_size=2,\n",
    "    sample_size=10,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5739e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85638e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
