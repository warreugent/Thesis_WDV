{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd772d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a4b0f-c1d3-424c-a46b-97b07f57d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac32a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab36a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_image_batches(images_folder, batch_size, sample_size=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Yield batches of images directly from a given folder.\n",
    "\n",
    "    Args:\n",
    "        images_folder (str | Path): Path to folder containing images (searched recursively).\n",
    "        batch_size (int): Number of images per batch.\n",
    "        sample_size (int, optional): Limit total number of images to sample.\n",
    "        random_state (int, optional): Seed for reproducible shuffling if needed.\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\")\n",
    "    paths = [p for p in Path(images_folder).glob(\"*\") if p.suffix.lower() in exts]\n",
    "    print(f\"Found {len(paths)} image files\")\n",
    "\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(paths)\n",
    "    if sample_size:\n",
    "        paths = paths[:sample_size]\n",
    "\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch = paths[i:i + batch_size]\n",
    "        yield [Image.open(p).convert(\"RGB\").copy() for p in batch], [p.name for p in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f60f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23220f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 image files\n",
      "Batch filenames: ['2025_02_26__14_24_30_468000000___camera_4__camera_5_bunch_5.png', '2025_02_06__13_28_13_350000000___camera_4__camera_5_bunch_12.png']\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "test_root = \"../Data/tomatoes/images/val\"   # adjust to your dataset path\n",
    "for imgs, names in retrieve_image_batches(test_root, batch_size=2, sample_size=4):\n",
    "    print(\"Batch filenames:\", names)\n",
    "    print(\"Batch size:\", len(imgs))\n",
    "    break  # only first batch for quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b65d9a-9962-420f-9437-56178225c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name):\n",
    "    if model_name == \"gd_t\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"gd_b\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_t\":\n",
    "        model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_b_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_base_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_l_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_large_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"])\n",
    "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"]).to(device)\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6327152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    dataset_folder,\n",
    "    categories_dict,\n",
    "    model_name,\n",
    "    batch_size=4,\n",
    "    sample_size=20,\n",
    "    random_state=44,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run zero-shot object detection and export results in COCO format.\n",
    "    Saves '<model_name>_<dataset_name>_predictions.json' inside the dataset folder.\n",
    "    \"\"\"\n",
    "    processor, model = select_model(model_name)\n",
    "\n",
    "    base = os.path.splitext(os.path.basename(dataset_folder))[0]\n",
    "    out_file = os.path.join(dataset_folder, f\"{model_name}_{base}_predictions.json\")\n",
    "\n",
    "    info = {\n",
    "        \"year\": 2025,\n",
    "        \"description\": f\"Predictions on {dataset_folder} with {model_name}\",\n",
    "        \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    categories = [{\"id\": cid, \"name\": name} for name, cid in categories_dict.items()]\n",
    "    images, annotations = [], []\n",
    "\n",
    "    def image_id_from_name(name: str) -> int:\n",
    "        return int(hashlib.md5(name.encode()).hexdigest()[:8], 16)\n",
    "\n",
    "    model.eval()\n",
    "    use_cuda = (model.device.type == \"cuda\")\n",
    "    label_list = list(categories_dict.keys())\n",
    "\n",
    "    # cache text tokens once (optional addition to improve inference)\n",
    "    cached_text = processor(text=[label_list], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    total_time = 0.0\n",
    "    num_images = 0\n",
    "    num_boxes = 0\n",
    "\n",
    "    # warmup (start timing after warmup to establish fair results)\n",
    "    for _ in range(2):\n",
    "        dummy = processor(\n",
    "            images=[Image.new(\"RGB\", (224, 224))] * batch_size,\n",
    "            text=[label_list] * batch_size,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        ).to(model.device)\n",
    "        with torch.inference_mode():\n",
    "            with torch.autocast(device_type=\"cuda\", enabled=use_cuda):\n",
    "                _ = model(**dummy)\n",
    "        if use_cuda:\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "    cached_text = None  # set before loop\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, names in retrieve_image_batches(\n",
    "            dataset_folder=dataset_folder,\n",
    "            batch_size=batch_size,\n",
    "            sample_size=sample_size,\n",
    "            random_state=random_state,\n",
    "        ):\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            # one-time text tokenization, included in timing\n",
    "            if cached_text is None:\n",
    "                cached_text = processor(text=[label_list], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "            # image preprocessing only\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\", padding=True)\n",
    "            inputs[\"input_ids\"] = cached_text.input_ids.repeat(len(imgs), 1)\n",
    "\n",
    "            # device transfer\n",
    "            inputs = {k: v.to(model.device, non_blocking=True) for k, v in inputs.items()}\n",
    "\n",
    "            # forward + sync\n",
    "            outputs = model(**inputs)\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # post-process\n",
    "            target_sizes = [(im.height, im.width) for im in imgs]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs[\"input_ids\"],\n",
    "                threshold=threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            end = time.perf_counter()\n",
    "            total_time += (end - start)\n",
    "            num_images += len(imgs)\n",
    "\n",
    "\n",
    "            for name, res in zip(names, results):\n",
    "                img_id = image_id_from_name(name)\n",
    "                images.append({\"id\": img_id, \"file_name\": f\"images/{name}\"})\n",
    "                for box, score, label in zip(res[\"boxes\"], res[\"scores\"], res.get(\"text_labels\", [])):\n",
    "                    x1, y1, x2, y2 = [float(v) for v in box.tolist()]\n",
    "                    cid = categories_dict.get(label)\n",
    "                    if cid:\n",
    "                        annotations.append({\n",
    "                            \"image_id\": img_id,\n",
    "                            \"category_id\": cid,\n",
    "                            \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                            \"score\": float(score.item()),\n",
    "                        })\n",
    "                        num_boxes += 1\n",
    "\n",
    "    avg_time_image = total_time / num_images if num_images else 0.0\n",
    "    avg_time_bbox = total_time / num_boxes if num_boxes else 0.0\n",
    "\n",
    "    info[\"num_images\"] = num_images\n",
    "    info[\"num_predicted_bbox\"] = num_boxes\n",
    "    info[\"avg_inference_time_s_image\"] = avg_time_image\n",
    "    info[\"avg_inference_time_s_bbox\"] = avg_time_bbox\n",
    "    info[\"total_inference_time_s\"] = total_time\n",
    "\n",
    "    coco_output = {\n",
    "        \"info\": info,\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories,\n",
    "    }\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "\n",
    "    print(f\"Wrote COCO-format JSON to {out_file}\")\n",
    "    print(f\"Total inference time: {total_time:.4f}s\")\n",
    "    print(f\"Avg inference time per image: {avg_time_image:.4f}s\")\n",
    "    print(f\"Avg inference time per bbox: {avg_time_bbox:.6f}s\")\n",
    "    print(f\"Processed {num_images} images and {num_boxes} boxes\")\n",
    "\n",
    "    return coco_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
