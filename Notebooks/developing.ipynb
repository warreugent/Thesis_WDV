{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92a4b0f-c1d3-424c-a46b-97b07f57d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/warredv/miniconda3/envs/thesis-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, infer_device\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab36a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the print takes a long time so inspect to use a native package or another solution to do this faster\n",
    "def retrieve_image_batches(images_folder, batch_size, sample_size=None, random_state=None):\n",
    "    \"\"\"\n",
    "    Yield batches of images directly from a given folder.\n",
    "\n",
    "    Args:\n",
    "        images_folder (str | Path): Path to folder containing images (searched recursively).\n",
    "        batch_size (int): Number of images per batch.\n",
    "        sample_size (int, optional): Limit total number of images to sample.\n",
    "        random_state (int, optional): Seed for reproducible shuffling if needed.\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".webp\")\n",
    "    paths = [p for p in Path(images_folder).glob(\"*\") if p.suffix.lower() in exts]\n",
    "    print(f\"Found {len(paths)} image files\")\n",
    "\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "        random.shuffle(paths)\n",
    "    if sample_size:\n",
    "        paths = paths[:sample_size]\n",
    "\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch = paths[i:i + batch_size]\n",
    "        yield [Image.open(p).convert(\"RGB\").copy() for p in batch], [p.name for p in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23220f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1325 image files\n",
      "Batch filenames: ['2025_02_06__13_24_55_176000000___camera_4__camera_5_bunch_3.png', '2023_11_22__14_52_11_503000000___04Z49__04T2Y_bunch_9.png']\n",
      "Batch size: 2\n"
     ]
    }
   ],
   "source": [
    "test_root = \"../Data/tomatoes/images/val\"   # adjust to your dataset path\n",
    "for imgs, names in retrieve_image_batches(test_root, batch_size=2, sample_size=4):\n",
    "    print(\"Batch filenames:\", names)\n",
    "    print(\"Batch size:\", len(imgs))\n",
    "    break  # only first batch for quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b65d9a-9962-420f-9437-56178225c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name):\n",
    "    if model_name == \"gd_t\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"gd_b\":\n",
    "        model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_t\":\n",
    "        model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_b_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_base_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    if model_name == \"mmgd_l_all\": # too big for T4 GPU\n",
    "        model_id = \"rziga/mm_grounding_dino_large_all\"\n",
    "        device = infer_device()\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"])\n",
    "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)#, token=os.environ[\"HF_TOKEN\"]).to(device)\n",
    "\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea0985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output_path(experiment, images_folder, model_name):\n",
    "    parent = os.path.basename(os.path.dirname(os.path.dirname(images_folder)))\n",
    "    base = os.path.splitext(os.path.basename(images_folder))[0]\n",
    "\n",
    "    out_dir = os.path.join(\"..\", \"Results\", experiment)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    out_file = os.path.join(out_dir, f\"{parent}_{base}_{model_name}_predictions.json\")\n",
    "    return out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a522833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_coco_output(\n",
    " images_folder,\n",
    " model_name,\n",
    " categories_list,\n",
    " images,\n",
    " annotations,\n",
    " num_images,\n",
    " num_boxes,\n",
    " total_time,\n",
    " gpu_hourly_price=2.5,\n",
    " gpu_tdp_watts=250.0,\n",
    " gpu_utilization_factor=0.9,\n",
    " power_utilization_factor=0.7,\n",
    " electricity_price_per_kwh=0.30,\n",
    "):\n",
    " # ---- timing statistics ----\n",
    " avg_time_image = total_time / num_images if num_images else 0.0\n",
    " avg_time_bbox = total_time / num_boxes if num_boxes else 0.0\n",
    "\n",
    " # ---- cost / energy estimates ----\n",
    " gpu_hours = (total_time / 3600.0) * gpu_utilization_factor\n",
    " infra_cost_eur = gpu_hours * gpu_hourly_price\n",
    " energy_kwh = (gpu_tdp_watts / 1000.0) * (total_time / 3600.0) * power_utilization_factor\n",
    " energy_cost_eur = energy_kwh * electricity_price_per_kwh\n",
    " total_cost_eur = infra_cost_eur + energy_cost_eur\n",
    "\n",
    "#  cost_per_image_eur = total_cost_eur / num_images if num_images else 0.0\n",
    " cost_per_bbox_eur = total_cost_eur / num_boxes if num_boxes else 0.0\n",
    "\n",
    " info = {\n",
    "     \"description\": f\"Predictions on {images_folder} with {model_name}\",\n",
    "     \"date_created\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "     \"num_images\": num_images,\n",
    "     \"num_predicted_bbox\": num_boxes,\n",
    "     \"avg_inference_time_s_image\": avg_time_image,\n",
    "     \"avg_inference_time_s_bbox\": avg_time_bbox,\n",
    "     \"total_inference_time_s\": total_time,\n",
    "     \"gpu_hours_estimate\": gpu_hours,\n",
    "     \"total_cost_eur\": total_cost_eur,\n",
    "     \"cost_per_bbox_eur\": cost_per_bbox_eur,\n",
    " }\n",
    "\n",
    " categories = [{\"id\": i + 1, \"name\": name} for i, name in enumerate(categories_list)]\n",
    "\n",
    " coco_output = {\n",
    "     \"info\": info,\n",
    "     \"images\": images,\n",
    "     \"annotations\": annotations,\n",
    "     \"categories\": categories,\n",
    " }\n",
    "\n",
    " out_file = build_output_path(\"Experiment_1\", images_folder, model_name)\n",
    " with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "     json.dump(coco_output, f, indent=2)\n",
    "\n",
    " return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6327152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zero_shot_predictions(\n",
    "    images_folder,\n",
    "    categories_list,\n",
    "    model_name,\n",
    "    batch_size,\n",
    "    sample_size,\n",
    "    random_state=None,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run zero-shot object detection and export results in COCO format.\n",
    "\n",
    "    This function:\n",
    "      1. Loads a model and processor via `select_model(model_name)`.\n",
    "      2. Iterates over images in `images_folder` in batches.\n",
    "      3. Runs zero-shot detection.\n",
    "      4. Collects predictions into COCO-style structures.\n",
    "      5. Calls `write_coco_output` to write a JSON file for Experiment_1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images_folder : str\n",
    "        Folder containing images.\n",
    "    categories_list : list[str]\n",
    "        Class names (e.g. [\"cat\", \"dog\", \"person\"]).\n",
    "    model_name : str\n",
    "        Model identifier for select_model().\n",
    "    batch_size : int\n",
    "        Number of images per batch.\n",
    "    sample_size : int\n",
    "        Total number of images to sample from the folder.\n",
    "    random_state : int or None, optional\n",
    "        Random seed used in `retrieve_image_batches`.\n",
    "    threshold : float, optional\n",
    "        Detection score threshold.\n",
    "    text_threshold : float, optional\n",
    "        Text matching threshold for the grounded detection head.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model and processor\n",
    "    processor, model = select_model(model_name)\n",
    "\n",
    "    # Map category name -> category id (1-based)\n",
    "    categories_dict = {name: i + 1 for i, name in enumerate(categories_list)}\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "\n",
    "    def image_id_from_name(name: str) -> int:\n",
    "        \"\"\"Stable integer id derived from the image filename.\"\"\"\n",
    "        return int(hashlib.md5(name.encode()).hexdigest()[:8], 16)\n",
    "\n",
    "    model.eval()\n",
    "    use_cuda = (model.device.type == \"cuda\")\n",
    "\n",
    "    # Labels used for the text encoder\n",
    "    label_list = categories_list\n",
    "    cached_text = None  # filled the first time we see a batch\n",
    "\n",
    "    total_time = 0.0\n",
    "    num_images = 0\n",
    "    num_boxes = 0\n",
    "\n",
    "    warmup_steps = 2\n",
    "    first_batch = True\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs, names in retrieve_image_batches(\n",
    "            images_folder=images_folder,\n",
    "            batch_size=batch_size,\n",
    "            sample_size=sample_size,\n",
    "            random_state=random_state,\n",
    "        ):\n",
    "            # Build and cache text encoding once\n",
    "            if cached_text is None:\n",
    "                cached_text = processor(\n",
    "                    text=[label_list],\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                )\n",
    "\n",
    "            # Prepare model inputs for this batch\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\", padding=True)\n",
    "            inputs[\"input_ids\"] = cached_text.input_ids.repeat(len(imgs), 1)\n",
    "            inputs = {\n",
    "                k: v.to(model.device, non_blocking=True)\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "\n",
    "            # Warmup on the first batch (not timed)\n",
    "            if first_batch:\n",
    "                for _ in range(warmup_steps):\n",
    "                    _ = model(**inputs)\n",
    "                    if use_cuda:\n",
    "                        torch.cuda.synchronize()\n",
    "                first_batch = False\n",
    "\n",
    "            # Timed forward pass\n",
    "            start = time.perf_counter()\n",
    "            outputs = model(**inputs)\n",
    "            if use_cuda:\n",
    "                torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "\n",
    "            total_time += (end - start)\n",
    "            num_images += len(imgs)\n",
    "\n",
    "            # Post-process detections to image coordinates\n",
    "            target_sizes = [(im.height, im.width) for im in imgs]\n",
    "            results = processor.post_process_grounded_object_detection(\n",
    "                outputs,\n",
    "                inputs[\"input_ids\"],\n",
    "                threshold=threshold,\n",
    "                text_threshold=text_threshold,\n",
    "                target_sizes=target_sizes,\n",
    "            )\n",
    "\n",
    "            # Collect COCO-style image and annotation entries\n",
    "            for name, res, im in zip(names, results, imgs):\n",
    "                img_id = image_id_from_name(name)\n",
    "                H, W = im.height, im.width\n",
    "\n",
    "                images.append({\n",
    "                    \"id\": img_id,\n",
    "                    \"file_name\": f\"images/val/{name}\",\n",
    "                })\n",
    "\n",
    "                boxes = res[\"boxes\"].tolist()\n",
    "                scores = res[\"scores\"].tolist()\n",
    "                labels = res.get(\"text_labels\", [])\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = map(float, box)\n",
    "\n",
    "                    # Clamp to image bounds\n",
    "                    x1 = max(0.0, min(x1, W))\n",
    "                    y1 = max(0.0, min(y1, H))\n",
    "                    x2 = max(0.0, min(x2, W))\n",
    "                    y2 = max(0.0, min(y2, H))\n",
    "\n",
    "                    # Enforce correct ordering\n",
    "                    if x2 < x1:\n",
    "                        x1, x2 = x2, x1\n",
    "                    if y2 < y1:\n",
    "                        y1, y2 = y2, y1\n",
    "\n",
    "                    w = max(0.0, x2 - x1)\n",
    "                    h = max(0.0, y2 - y1)\n",
    "                    if w == 0.0 or h == 0.0:\n",
    "                        continue\n",
    "\n",
    "                    # Map label string to category id\n",
    "                    cid = categories_dict.get(label)\n",
    "                    if cid is None:\n",
    "                        continue\n",
    "\n",
    "                    ann_id = len(annotations) + 1\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": cid,\n",
    "                        \"bbox\": [x1, y1, w, h],\n",
    "                        \"score\": float(score),\n",
    "                    })\n",
    "                    num_boxes += 1\n",
    "\n",
    "    # Write COCO JSON and log path\n",
    "    out_file = write_coco_output(\n",
    "        images_folder=images_folder,\n",
    "        model_name=model_name,\n",
    "        categories_list=categories_list,\n",
    "        images=images,\n",
    "        annotations=annotations,\n",
    "        num_images=num_images,\n",
    "        num_boxes=num_boxes,\n",
    "        total_time=total_time,\n",
    "    )\n",
    "\n",
    "    print(f\"Wrote COCO-format JSON to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38602792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1325 image files\n",
      "Wrote COCO-format JSON to ../Results/Experiment_1/tomatoes_val_gd_t_predictions.json\n"
     ]
    }
   ],
   "source": [
    "make_zero_shot_predictions(\n",
    "    images_folder=\"../Data/tomatoes/images/val\",\n",
    "    categories_list=[\"tomato\"],\n",
    "    model_name=\"gd_t\",\n",
    "    batch_size=2,\n",
    "    sample_size=4,\n",
    "    threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
