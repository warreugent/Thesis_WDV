{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5454e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import annotations\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from __future__ import annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe2f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _xywh_to_xyxy(b):\n",
    "    x, y, w, h = b\n",
    "    return (x, y, x + w, y + h)\n",
    "\n",
    "def _iou(b1, b2):\n",
    "    x1,y1,x2,y2 = _xywh_to_xyxy(b1)\n",
    "    x1g, y1g, x2g, y2g = _xywh_to_xyxy(b2)\n",
    "    ix1, iy1 = max(x1, x1g), max(y1, y1g)\n",
    "    ix2, iy2 = min(x2, x2g), min(y2, y2g)\n",
    "    iw, ih = max(0.0, ix2 - ix1), max(0.0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    if inter <= 0:\n",
    "        return 0.0\n",
    "    area1 = (x2 - x1) * (y2 - y1)\n",
    "    area2 = (x2g - x1g) * (y2g - y1g)\n",
    "    union = area1 + area2 - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def compute_pr_manual(\n",
    "    gt_annotations,         # list of dicts: {\"image_id\", \"category_id\", \"bbox\"}\n",
    "    pred_annotations,       # list of dicts: {\"image_id\", \"category_id\", \"bbox\", \"score\"}\n",
    "    iou_thr=0.50,\n",
    "    image_ids_eval=None     # optional: restrict to these GT image_ids\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns: dict with TP, FP, FN, precision, recall\n",
    "    Notes:\n",
    "      - Greedy one-to-one matching per (image_id, category_id)\n",
    "      - All predictions considered (no maxDets)\n",
    "      - All gt considered (no area/crowd filtering)\n",
    "    \"\"\"\n",
    "\n",
    "    # group GT and predictions by (image_id, category_id)\n",
    "    gts = defaultdict(list)\n",
    "    dts = defaultdict(list)\n",
    "\n",
    "    if image_ids_eval is not None:\n",
    "        image_ids_eval = set(image_ids_eval)\n",
    "\n",
    "    for g in gt_annotations:\n",
    "        if image_ids_eval is not None and g[\"image_id\"] not in image_ids_eval:\n",
    "            continue\n",
    "        gts[(g[\"image_id\"], g[\"category_id\"])].append({\"bbox\": g[\"bbox\"], \"matched\": False})\n",
    "\n",
    "    for d in pred_annotations:\n",
    "        if image_ids_eval is not None and d[\"image_id\"] not in image_ids_eval:\n",
    "            continue\n",
    "        dts[(d[\"image_id\"], d[\"category_id\"])].append({\"bbox\": d[\"bbox\"], \"score\": float(d.get(\"score\", 1.0))})\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # iterate over all keys present in either GT or preds\n",
    "    keys = set(gts.keys()) | set(dts.keys())\n",
    "    for key in keys:\n",
    "        gt_list = gts.get(key, [])\n",
    "        dt_list = dts.get(key, [])\n",
    "\n",
    "        # sort detections by score desc\n",
    "        dt_list.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        gt_matched = [False] * len(gt_list)\n",
    "\n",
    "        # greedy matching\n",
    "        for det in dt_list:\n",
    "            best_iou = 0.0\n",
    "            best_j = -1\n",
    "            for j, gt in enumerate(gt_list):\n",
    "                if gt_matched[j]:\n",
    "                    continue\n",
    "                iou = _iou(det[\"bbox\"], gt[\"bbox\"])\n",
    "                if iou >= iou_thr and iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_j = j\n",
    "            if best_j >= 0:\n",
    "                gt_matched[best_j] = True\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        # any unmatched GT are FN\n",
    "        fn += sum(1 for m in gt_matched if not m)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0adc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions_dir: str | Path, iou_thr = 0.5, iou_type: str = \"bbox\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Expects prediction files named: {dataset}_{subset}_{model}_predictions.json\n",
    "    Writes evaluations to: ../Evaluations/{PREDICTIONS_DIR_NAME}/{dataset}_{subset}_{model}_evaluation.json\n",
    "    Requires:\n",
    "      - Ground truth at ../Data/{dataset}/annotations/instances_{subset}.json\n",
    "      - Optional: preds[\"images\"], preds[\"categories\"], preds[\"annotations\"]\n",
    "      - Optional: preds[\"total_inference_time_s\"]\n",
    "    Also relies on a user-defined compute_pr_manual(...) helper.\n",
    "    \"\"\"\n",
    "    predictions_dir = Path(predictions_dir)\n",
    "    out_root = Path(\"..\") / \"Evaluations\" / predictions_dir.name\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    json_files = sorted(predictions_dir.glob(\"*_predictions.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No prediction files found in {predictions_dir}\")\n",
    "        return []\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    for pred_path in json_files:\n",
    "        # ---- infer dataset, subset, model from filename ----\n",
    "        stem = pred_path.stem  # e.g., dataset_subset_model_predictions\n",
    "        parts = stem.split(\"_\")\n",
    "\n",
    "        if len(parts) < 3 or parts[-1] != \"predictions\":\n",
    "            # Fallback: try to find 'predictions' token and parse before it\n",
    "            try:\n",
    "                pred_idx = parts.index(\"predictions\")\n",
    "                core = parts[:pred_idx]\n",
    "            except ValueError:\n",
    "                core = parts\n",
    "            if len(core) < 3:\n",
    "                print(f\"Skip unrecognized filename pattern: {pred_path.name}\")\n",
    "                continue\n",
    "            dataset_name, subset_name = core[0], core[1]\n",
    "            model_name = \"_\".join(core[2:])\n",
    "        else:\n",
    "            dataset_name = parts[0]\n",
    "            subset_name = parts[1]\n",
    "            model_name = \"_\".join(parts[2:-1])  # anything between subset and 'predictions'\n",
    "\n",
    "        gt_path = Path(\"..\") / \"Data\" / dataset_name / \"annotations\" / f\"instances_{subset_name}.json\"\n",
    "        if not gt_path.exists():\n",
    "            print(f\"Missing GT: {gt_path}\")\n",
    "            continue\n",
    "\n",
    "        cocoGt = COCO(str(gt_path))\n",
    "\n",
    "        with open(pred_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            preds = json.load(f)\n",
    "\n",
    "        # --- map GT and prediction images by filename ---\n",
    "        gt_name_to_id = {os.path.basename(im[\"file_name\"]): im[\"id\"] for im in cocoGt.dataset[\"images\"]}\n",
    "        pred_images = preds.get(\"images\", [])\n",
    "        pred_id_to_name = {im[\"id\"]: os.path.basename(im[\"file_name\"]) for im in pred_images}\n",
    "\n",
    "        # image set = ALL images listed in preds[\"images\"] that exist in GT\n",
    "        img_ids_eval = sorted({\n",
    "            gt_name_to_id[os.path.basename(im[\"file_name\"])]\n",
    "            for im in pred_images\n",
    "            if os.path.basename(im[\"file_name\"]) in gt_name_to_id\n",
    "        })\n",
    "\n",
    "        # If no overlap, output zeros and continue\n",
    "        if not img_ids_eval:\n",
    "            empty = {k: 0.0 for k in [\n",
    "                \"AP@[.50:.95]\", \"AP@0.50\", \"AP@0.75\", \"AP_small\", \"AP_medium\", \"AP_large\",\n",
    "                \"AR@1\", \"AR@10\", \"AR@100\", \"AR_small\", \"AR_medium\", \"AR_large\"\n",
    "            ]}\n",
    "            eval_summary = {\n",
    "                \"info\": {\n",
    "                    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"dataset_name\": dataset_name,\n",
    "                    \"subset_name\": subset_name,\n",
    "                    \"model_name\": model_name,\n",
    "                    \"iou_type\": iou_type,\n",
    "                    \"num_predicted_bbox\": 0,\n",
    "                    \"num_gt_bbox\": 0,\n",
    "                    \"num_eval_images\": 0,\n",
    "                    \"num_eval_categories\": 0,\n",
    "                },\n",
    "                \"metrics\": empty,\n",
    "            }\n",
    "            eval_json = out_root / f\"{dataset_name}_{subset_name}_{model_name}_evaluation.json\"\n",
    "            with open(eval_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(eval_summary, f, indent=2)\n",
    "            results.append(eval_summary)\n",
    "            continue\n",
    "\n",
    "        # --- category remap ---\n",
    "        gt_cat_by_name = {c[\"name\"]: c[\"id\"] for c in cocoGt.dataset[\"categories\"]}\n",
    "        pred_cat_map: Dict[int, int] = {}\n",
    "        if \"categories\" in preds:\n",
    "            pred_cat_by_name = {c[\"name\"]: c[\"id\"] for c in preds[\"categories\"]}\n",
    "            # map prediction cat IDs -> GT cat IDs when names match but IDs differ\n",
    "            pred_cat_map = {\n",
    "                pred_cat_by_name[n]: gt_cat_by_name[n]\n",
    "                for n in (pred_cat_by_name.keys() & gt_cat_by_name.keys())\n",
    "                if pred_cat_by_name[n] != gt_cat_by_name[n]\n",
    "            }\n",
    "            cat_ids_eval = sorted({gt_cat_by_name[n] for n in (pred_cat_by_name.keys() & gt_cat_by_name.keys())})\n",
    "        else:\n",
    "            cat_ids_eval = sorted(gt_cat_by_name.values())\n",
    "\n",
    "        # --- remap detections (may be empty for some or all images) ---\n",
    "        remapped = []\n",
    "        for a in preds.get(\"annotations\", []):\n",
    "            pred_name = pred_id_to_name.get(a[\"image_id\"])\n",
    "            if not pred_name:\n",
    "                continue\n",
    "            gt_img_id = gt_name_to_id.get(pred_name)\n",
    "            if gt_img_id is None:\n",
    "                continue\n",
    "            cat_id = pred_cat_map.get(a[\"category_id\"], a[\"category_id\"])\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            remapped.append({\n",
    "                \"image_id\": gt_img_id,\n",
    "                \"category_id\": cat_id,\n",
    "                \"bbox\": [float(x), float(y), float(w), float(h)],\n",
    "                \"score\": float(a.get(\"score\", 1.0)),\n",
    "            })\n",
    "\n",
    "        # --- run COCOeval over the chosen image set, even if remapped is empty ---\n",
    "        cocoDt = cocoGt.loadRes(remapped)  # [] allowed\n",
    "        cocoEval = COCOeval(cocoGt, cocoDt, iouType=iou_type)\n",
    "        cocoEval.params.imgIds = img_ids_eval\n",
    "        cocoEval.params.catIds = cat_ids_eval\n",
    "\n",
    "        cocoEval.evaluate()\n",
    "        cocoEval.accumulate()\n",
    "        cocoEval.summarize()\n",
    "\n",
    "        metrics = {\n",
    "            \"AP@[.50:.95]\": cocoEval.stats[0],\n",
    "            \"AP@0.50\":      cocoEval.stats[1],\n",
    "            \"AP@0.75\":      cocoEval.stats[2],\n",
    "            \"AP_small\":     cocoEval.stats[3],\n",
    "            \"AP_medium\":    cocoEval.stats[4],\n",
    "            \"AP_large\":     cocoEval.stats[5],\n",
    "            \"AR@1\":         cocoEval.stats[6],\n",
    "            \"AR@10\":        cocoEval.stats[7],\n",
    "            \"AR@100\":       cocoEval.stats[8],\n",
    "            \"AR_small\":     cocoEval.stats[9],\n",
    "            \"AR_medium\":    cocoEval.stats[10],\n",
    "            \"AR_large\":     cocoEval.stats[11],\n",
    "        }\n",
    "\n",
    "        # --- additional metrics ---\n",
    "        num_gt_bbox = sum(1 for a in cocoGt.dataset[\"annotations\"] if a[\"image_id\"] in img_ids_eval)\n",
    "        num_predicted_bbox = len(remapped)\n",
    "\n",
    "        # User-provided helper must exist\n",
    "        res = compute_pr_manual(\n",
    "            cocoGt.dataset[\"annotations\"],\n",
    "            remapped,\n",
    "            iou_thr=iou_thr,\n",
    "            image_ids_eval=img_ids_eval\n",
    "        )\n",
    "\n",
    "        bbox_additions = res[\"fn\"]\n",
    "        bbox_removals  = res[\"fp\"]\n",
    "        precision_iou   = res[\"precision\"]\n",
    "        recall_iou      = res[\"recall\"]\n",
    "\n",
    "        metrics.update({\n",
    "            \"bbox_additions\": bbox_additions,\n",
    "            \"bbox_removals\": bbox_removals,\n",
    "            \"precision@%.2f\" % iou_thr: precision_iou,\n",
    "            \"recall@%.2f\" % iou_thr: recall_iou,\n",
    "        })\n",
    "\n",
    "        # time calculations\n",
    "        total_inference_time = preds.get(\"info\", {}).get(\"total_inference_time_s\", 0.0)        \n",
    "        total_annotation_time = total_inference_time + bbox_additions * 10.15 + bbox_removals * 5.20\n",
    "        annotation_time_per_bbox = total_annotation_time / num_gt_bbox if num_gt_bbox > 0 else 0.0\n",
    "\n",
    "        metrics[\"total_annotation_time_s\"] = total_annotation_time\n",
    "        metrics[\"annotation_time_per_bbox_s\"] = annotation_time_per_bbox\n",
    "\n",
    "        # cost calculations\n",
    "        cost_per_bbox_eur = preds.get(\"info\", {}).get(\"cost_per_bbox_eur\", 0.0)\n",
    "        metrics[\"cost_per_bbox_eur\"] = cost_per_bbox_eur\n",
    "\n",
    "        eval_summary = {\n",
    "            \"info\": {\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"subset_name\": subset_name,\n",
    "                \"model_name\": model_name,\n",
    "                \"iou_type\": iou_type,\n",
    "                \"num_predicted_bbox\": num_predicted_bbox,\n",
    "                \"num_gt_bbox\": num_gt_bbox,\n",
    "                \"num_eval_images\": len(img_ids_eval),\n",
    "                \"num_eval_categories\": len(cat_ids_eval),\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "\n",
    "        # write JSON summary next to predictions\n",
    "        eval_json = out_root / f\"{dataset_name}_{subset_name}_{model_name}_evaluation.json\"\n",
    "        with open(eval_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(eval_summary, f, indent=2)\n",
    "\n",
    "        results.append(eval_summary)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaced87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.299\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.253\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.396\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.081\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'info': {'timestamp': '2025-11-23 13:25:43',\n",
       "   'dataset_name': 'tomatoes',\n",
       "   'subset_name': 'val',\n",
       "   'model_name': 'gd_t',\n",
       "   'iou_type': 'bbox',\n",
       "   'num_predicted_bbox': 10,\n",
       "   'num_gt_bbox': 26,\n",
       "   'num_eval_images': 4,\n",
       "   'num_eval_categories': 1},\n",
       "  'metrics': {'AP@[.50:.95]': np.float64(0.24947666195190948),\n",
       "   'AP@0.50': np.float64(0.299009900990099),\n",
       "   'AP@0.75': np.float64(0.2527581329561528),\n",
       "   'AP_small': np.float64(0.07920792079207918),\n",
       "   'AP_medium': np.float64(0.3962871287128713),\n",
       "   'AP_large': np.float64(-1.0),\n",
       "   'AR@1': np.float64(0.08076923076923076),\n",
       "   'AR@10': np.float64(0.26153846153846155),\n",
       "   'AR@100': np.float64(0.26153846153846155),\n",
       "   'AR_small': np.float64(0.07272727272727272),\n",
       "   'AR_medium': np.float64(0.4),\n",
       "   'AR_large': np.float64(-1.0),\n",
       "   'bbox_additions': 18,\n",
       "   'bbox_removals': 2,\n",
       "   'precision@0.50': 0.8,\n",
       "   'recall@0.50': 0.3076923076923077,\n",
       "   'total_annotation_time_s': 283.3155641439995,\n",
       "   'annotation_time_per_bbox_s': 10.896752467076904,\n",
       "   'cost_per_bbox_eur': 0.005770037123376632}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_predictions(predictions_dir=\"../Results/Experiment_1/\", iou_thr=0.5, iou_type=\"bbox\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
